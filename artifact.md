# Artifact for "Abstract Interpretation of Temporal Safety Effects of Higher Order Programs"

This artifact includes the instructions to run the experiments and reproduce the results presented in Table 1 (Section 8) and Table 2 (Appendix F).

We claim the available and reusable badges for `evDrift`.

We use BenchExec https://github.com/sosy-lab/benchexec/), which is a nearly industral-level
benchmarking tool, that uses Linux CGroups to measure CPU and memory
usage with high precision. We use BenchExec to run Drift and evDrift across
all possible configurations, as well as MoCHi, Rethfl and RCaml.

NOTE: If you are interested in generating a simplified variant of the Table 1 and Table 2 output,
the alternative, option is likely to follow the instructions described 
in the "B. Simplified" section, shown at the end, for running evDrift and the tools above inside a Docker container. 

## Hardware requirements

BenchExec requires systems with cgroups v2. 

The simplified variant requires `Docker` to be installed on your machine. Documentation can be found here:  https://docs.docker.com/engine/install/ 


## A. Precise Results 

(Again, note that this involves the use of BenchExec. If you are looking to simply run the various tools without intensive, precise benchmarking, you may want to skip to section B". Simplified" below.)

### 1. Install dependencies

1. Install BenchExec, per the instructions found at:
https://github.com/sosy-lab/benchexec

2. Install Perl modules:

```
cpan -i Statistics::Basic
cpan -i List::Util
cpan -i Math::Complex
cpan -i DateTime
cpan -i Time::ParseDate
```

### 2. Set up BenchExec

* In the following, we assume that environment variable $DRIFT_REPO
is set to the Drift repository root folder. For example:
```
export DRIFT_REPO=~/drift
```

* If you haven't already, build Drift (after installing dependences per the root README.md)
```
cd $DRIFT_REPO
dune build
```

* Symlink drift so it's available in `/usr`:
```
sudo ln -s $DRIFT_REPO/drift.exe /usr/local/bin
```
* Modify the BenchExec toolinfo modules to point to the Drift, MoCHI, and RCaml binaries:
```
nano $DRIFT_REPO/scripts/benchexec-drifttoolinfo/drifttoolinfo/coarmochi.py
nano $DRIFT_REPO/scripts/benchexec-drifttoolinfo/drifttoolinfo/mochi.py
nano $DRIFT_REPO/scripts/benchexec-drifttoolinfo/drifttoolinfo/drift.py
nano $DRIFT_REPO/scripts/benchexec-drifttoolinfo/drifttoolinfo/driftwrapper.py
nano $DRIFT_REPO/scripts/benchexec-drifttoolinfo/drifttoolinfo/rethfl.py
```
* Build and install the BenchExec toolinfo modules:

```
cd $DRIFT_REPO/scripts/benchexec-drifttoolinfo
make
```

### 3. Running benchmarking

Note that many of these procedures below have Makefile targets. See `$DRIFT_REPO/scripts/effects/Makefile`

* Benchmarking Drift and evDrift across all possible configuration parameters (makefile target `best_config_csvs`). NOTE: This is very expensive. The following
will generate many possible run definitions and then use BenchExec to
execute drift and evDrift on them:

```
cd $DRIFT_REPO/scripts/effects
perl makeYamls.pl
perl makeRunDefinitions.pl
benchexec benchmark-drift-autogenerated.xml @benchexec.cfg
benchexec benchmark-evdrift-autogenerated.xml @benchexec.cfg
```

Once that completes you must harvest the results as follows. 

* Go into scripts/effects/ and run the TWO `table-generator` commands 
as instructed by BenchExec above. Note: there are two commands to run,
one for Drift, one for evDrift.

* Go into scripts/effects/ and run:

```
perl makeBestConfigs.pl (names of the TWO generated .table.csv file above)
```

* Copy the resulting best_configs_[ev]drift.csvs into the root folder.

* Go into scripts/effects/ and run:

```
perl makeTracePart.pl (names of the TWO generated .table.csv file above)
```

* Copy the resulting tp(on|off)_best_configs_[ev]drift.csvs into the root folder.

* You can now benchmark Drift and evDrift using the best configs (saved to CSVs) generated in the previous step. (You can also now run drift/evDrift by consuming
the best config by running `./drift_benchmark.pl (csv file) test/effects/foo.ml`.)

* With these best_configs_[ev]drift.csv files, we can now benchmark drift/evDrift
along with the other state-of-the-art tools as follows.
(see also `make precise` and `make precise_results`)

```
cd $DRIFT_REPO/scripts/effects
benchexec benchmark-driftwrapper.xml @benchexec.cfg
```

* Benchmarking COaR/RCaml

Start by downloading the source code

```
git clone https://github.com/hiroshi-unno/coar/ $HOME/RCaml
```
and follow the corresponding instructions to compile it locally to produce `_build/default/main.exe`

Continue with BenchExec configuration.
(see also `make precise` and `make precise_results`)
```

cd $DRIFT_REPO/scripts/effects
benchexec benchmark-coarmochi.xml @benchexec.cfg
```

* Benchmarking MoChi

Start by downloading the source code
```
git clone https://github.com/hopv/MoCHi $HOME/MoCHi
```
and follow the instructions to produce the executable `src/mochi.exe`

Continue with BenchExec configuration.
(see also `make precise` and `make precise_results`)

```
cd $DRIFT_REPO/tests/effects/tr_tuple_mochi
benchexec benchmark-mochi.xml @benchexec.cfg
```

* Benchmarking Rethfl

Start by downloading the source code
```
git clone https://github.com/hopv/rethfl $HOME/ReTHFL
```
and follow the instructions for local compilation

Continue with BenchExec configuration.
(see also `make precise` and `make precise_results`)
```
benchexec benchmark-rethfl.xml @benchexec.cfg
```

The Mochi sources are already translated. If you wish to re-run translation use:
```
export MOCHI_EXE="$HOME/MoCHi/src/mochi.exe"
cd $DRIFT_REPO/scripts/effects/
perl ml_rethfl_translate.pl $MOCHI_EXE ../../tests/effects/tr_tuple_mochi/ ../../tests/effects/tr_tuple_hflz
```
### 4. Generating LaTeX results

After each `benchexec` above, you will be given a table-generator instruction to
run, which will produce CSVs for each tool. You will then have CSV files with
names like: `results/benchmark-coarmochi.2024-02-25_10-16-09.results.default.mochibenchmarks.csv`

After generating those CSVs, put these CSV file names into `scripts/effects/csvs.txt`. The final step consumes these CSVs: (See also `make precise_results`.)


```
cd scripts/effects
perl makeLatex.pl csvs.txt
```

This will generate, e.g., exp-body.tex.


## B. Simplified

First, you need to make sure `Docker` is installed on your machine. Documentation can be found here:  https://docs.docker.com/engine/install/

### Mac users
Use Docker Desktop version v4.34.3 or later, and enable "Use Rosetta for x86_64/amd64 emulation on Apple Silicon" in Settings->General.

The artifact uses Docker to run all experiments. Ensure you have at least 20GB of available disk space. The uncompressed Docker image is approximately 8GB and includes the Git repositories used to build all tools included in the comparison. 

A compressed archive containing the pre-built Docker image is available at Zenodo as paper750.tar.gz. Use the following command to extract it:
```bash
tar -xzvf paper750.tar.gz
```

Load the docker image `oopsla25-evdrift` from the tar archive
```bash
docker load -i oopsla25-evdrift.tar
```

Run the container with
```bash
docker run --init -d oopsla25-evdrift tail -f /dev/null
```
This command starts and keep the container running indefinitely in the background. You can see its name by executing `docker container ls` and attach to it using:
```bash
docker exec -it <container_id_or_name> /bin/bash
```
Once you are inside the container--the above command places you in a bash environment--you can inspect the source code and run the experiments. 

When finished, you can stop and remove the container by passing the command `docker rm -f <container_id_or_name>`.

## Smoke test

To run a subset of the experiments, use 
```bash
python3 run_expeeriments.py smoke
```
This will detect any technical issues during the smoke test phase. 
If finished successfully, the evaluation script should print
```
name        drift   rcaml   mochi   rethfl  evDrift
---------------------------------------------------
all-ev-pos  verif.  verif.  verif.  verif.  verif.
depend      verif.  verif.  verif.  verif.  verif.
overview1   verif.  verif.  verif.  verif.  verif.
```

## Running full experiments 

The sources can be found in `/oopsla25/evDrift/tests/effects/*.ml`, along with properties
in `/oopsla25/evDrift/tests/effects/*.yml.prp`. Unsafe versions of the benchmarks are 
found in `/oopsla25/evDrift/tests/effects/unsafe/*.ml`.

The scripts to re-produce the experiments that are reported in Table 1 are described below. Note that we have used BenchExec
(https://github.com/sosy-lab/benchexec/), which is a nearly industral-level benchmarking tool, that uses Linux CGroups to measure CPU and memory usage with high precision. We use BenchExec to run Drift and evDrift across all possible configurations. If interested, these full experiments can be reproduced according to the instructions found in
`/oopsla25/evDrift/scripts/effects/README.md`.

To avoid the need to install and configure BenchExec, we have created
a script that simply run the best-performing configurations, which are those reported in Table 1. To reproduce the results in the paper, run:

```
python3 run_experiments.py all
```

The output for each run can be found at: `/oopsla25/evDrift/result/<benchmark_name>_<tool_name>.txt`.

Note that the 900-second timeout per run causes the full experiment to take around 4 hours.

You can exit the container by typing `exit`. After exiting the container, the command ``docker rm <container_id_or_name>`` can be used to remove the container. 

## Reusability
The source code for `evDrift` is located in `/oopsla25/evDrift` directory of the container and on Github at https://github.com/nyu-acsys/drift

See the `README.md` file inside the source directory for complete installation instructions across platforms and guidance on running the tool with different configuration options.


